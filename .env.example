# RAG Document Assistant - Environment Configuration
# Copy this file to .env and update with your settings
 
# ====================================
# LLM Provider Configuration
# ====================================
# Options: "openai", "ollama", "huggingface"
LLM_PROVIDER=ollama
 
# Model name (depends on provider)
# OpenAI: gpt-3.5-turbo, gpt-4, etc.
# Ollama: llama2, mistral, codellama, etc.
# HuggingFace: google/flan-t5-base, etc.
LLM_MODEL_NAME=llama2
 
# Generation temperature (0.0 to 1.0)
LLM_TEMPERATURE=0.7
 
# Maximum tokens in response
LLM_MAX_TOKENS=512
 
# ====================================
# OpenAI Configuration (if using OpenAI)
# ====================================
# Get your API key from https://platform.openai.com/api-keys
OPENAI_API_KEY=
 
# ====================================
# Ollama Configuration (if using Ollama)
# ====================================
# Local Ollama server URL
OLLAMA_BASE_URL=http://localhost:11434
 
# ====================================
# Embedding Model Configuration
# ====================================
# HuggingFace model for embeddings
EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
 
# Embedding vector dimension (depends on model)
EMBEDDING_DIMENSION=384
 
# ====================================
# Text Chunking Configuration
# ====================================
# Maximum characters per chunk
CHUNK_SIZE=1000
 
# Overlap between chunks (for context continuity)
CHUNK_OVERLAP=200
 
# ====================================
# Processing Configuration
# ====================================
# Batch size for embedding generation
BATCH_SIZE=32
 
# Device for model inference ("cpu" or "cuda")
DEVICE=cpu
 
# ====================================
# Storage Configuration
# ====================================
# Path to vector database
VECTOR_DB_PATH=./data/vector_db
 
# ChromaDB collection name
COLLECTION_NAME=document_embeddings
 
# Path for uploaded PDFs
PDF_STORAGE_PATH=./data/pdfs